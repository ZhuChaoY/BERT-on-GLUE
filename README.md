# BERT-on-GLUE
An implementation of BERT finetuning on GLUE dataset by tensorflow.

## Reference
(1) **BERT**: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) (Code: https://github.com/google-research/bert)   
(2) **GLUE**: [GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTANDING](https://arxiv.org/pdf/1804.07461v2.pdf)

## File
# Pretrained BERT
base/  : download from https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip  
large/ : download from https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip  
# GLUE
MNLI/ : download the train.txt from https://gluebenchmark.com/tasks  
QQP/ : upzip the train.zip  
**ELSE**/ : READY
